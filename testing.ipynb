{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Links\n",
    "- Examples: https://docs.google.com/spreadsheets/d/14pdiODt9PmsD2F___Lpv2rO-RByTtC8PYcEfxL8iIuY/edit#gid=186467173\n",
    "- LangChain: https://github.com/hwchase17/langchain\n",
    "- Chroma: no docker or API_KEY - https://www.trychroma.com/\n",
    "- OpenAI account usage: https://platform.openai.com/account/usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- **Eric & Jacob**: Validation (`validate_stages.py` is not yet working, and has not been incorporated)\n",
    "- **Jacob**: Label class recognition\n",
    "    - determine what classes the user is referring to for each label field\n",
    "- **Allen**: Evaluation Identification Stage:\n",
    "    - `links/evaluation_run_selector.py` \n",
    "    - add stage to `dataset_view_generator.py`\n",
    "    - add `prompts/evaluation_task_rules.txt` prompt\n",
    "    - add examples to Examples spreadsheet in new tab, and then put in `examples` folder\n",
    "- **Leila**: Add support for `hardness` brain runs\n",
    "    - fill out the template in `links/brain_run_selector.py` \n",
    "    - add `prompts/hardness_task_rules.txt` prompt modeled after uniqueness and mistakenness.\n",
    "    - new examples tab `hardness` modeled after `uniqueness` and `mistakenness` tabs and then put in `examples` folder\n",
    "- **Vini**: More examples\n",
    "    - scrape examples from community Slack covering as wide a range of scenarios as possible\n",
    "    - complex filters and view expressions!\n",
    "    - videos\n",
    "    - every type of label\n",
    "    - varied naming conventions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes\n",
    "\n",
    "- Want to validate embedded fields\n",
    "- At present, there is no support for multi-line Python code to generate views. It all needs to be done inline\n",
    "- No support for groups, 3D/point clouds, `concat()`, or `mongo()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install openai langchain chroma`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create an OpenAI account and generate an API key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`export OPENAI_API_KEY=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from links.view_stage_example_selector import generate_view_stage_examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five random images from the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stage_examples_prompt = generate_view_stage_examples_prompt(dataset, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(view_stage_examples_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate View Stage Descriptions Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.view_stage_description_selector import generate_view_stage_descriptions_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stage_descriptions_prompt = generate_view_stage_descriptions_prompt(view_stage_examples_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(view_stage_descriptions_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain method selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.brain_method_selector import select_brain_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five most unique images from the dataset\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"50 least unique images from the dataset that were hard\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five most mistaken images from the dataset\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain run selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.brain_run_selector import select_brain_runs\n",
    "import fiftyone.brain as fob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to create brain runs if you don't have them already\n",
    "# fob.compute_similarity(\n",
    "#     dataset,\n",
    "#     model=\"clip-vit-base32-torch\",\n",
    "#     brain_key=\"img_sim\",\n",
    "# )\n",
    "# fob.compute_similarity(\n",
    "#     dataset,\n",
    "#     patches_field=\"ground_truth\",\n",
    "#     model=\"resnet18-imagenet-torch\",\n",
    "#     brain_key=\"gt_sim\",\n",
    "# )\n",
    "# fob.compute_uniqueness(dataset)\n",
    "\n",
    "# fob.compute_mistakenness(dataset, \"predictions\", label_field=\"ground_truth\")\n",
    "\n",
    "# Create some fake classifications to make a hardness brain run\n",
    "# import random\n",
    "# import numpy as np\n",
    "# classes = [\"sheep\", \"cat\", \"dog\", \"moose\"]\n",
    "# logits = np.random.normal(size = 4)\n",
    "# logits /= logits.sum()\n",
    "# for sample in dataset:\n",
    "#     sample[\"my_classifications\"] = fo.Classification(label=random.choice(classes), logits=logits, confidence=random.random())\n",
    "#     sample.save()\n",
    "\n",
    "# fob.compute_hardness(dataset, label_field=\"my_classifications\")\n",
    "# fob.compute_hardness(dataset, label_field=\"my_classifications\", hardness_field=\"test_hardness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"most unique images from the dataset with key 'uniqueness1 that are similar to image 10\"\n",
    "select_brain_runs(dataset, query, [\"uniqueness\", \"image_similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five most mistaken images from the dataset with ground truth field 'predictions\"\n",
    "select_brain_runs(dataset, query, [\"mistakenness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five hardest images from dataset\"\n",
    "select_brain_runs(dataset, query, [\"hardness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hardness from key 'test_hardness'\"\n",
    "select_brain_runs(dataset, query, [\"hardness\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.field_selector import select_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sort pred1 by number of detections\"\n",
    "fields = select_fields(dataset, query)\n",
    "print(fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label class selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Loading existing dataset 'quickstart'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.label_class_selector import select_label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Bear with bear\n",
      "Class name glove not found for label predictions\n",
      "{'predictions': ['bear', 'glove']}\n"
     ]
    }
   ],
   "source": [
    "query = \"sort Bear and glove predictions by number of detections\"\n",
    "prompt = select_label_classes(dataset, query, \"[predictions]\")\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetView generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_view_generator import get_gpt_view_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"images with tables\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"high confidence detections\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"first 30 images with a ground truth detection\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: sort by image uniqueness1 for images with a >0.5 confidence classification\n",
      "Identified likely view stages: ['exists', 'match', 'sort_by', 'sort_by_similarity', 'filter_labels', 'filter_keypoints', 'match_labels', 'skip', 'exclude', 'exclude_by']\n",
      "Identified brain methods: ['uniqueness']\n",
      "Identified brain runs: {'uniqueness': 'uniqueness1'}\n",
      "Identified potentially relevant fields: [predictions, mistakenness1_eval_fp]\n",
      "Identified label classes: {'predictions': []}\n",
      "[match_labels(\n",
      "    filter=F(\"predictions.confidence\") > 0.5,\n",
      "    fields=\"ground_truth\"\n",
      "), sort_by(\"uniqueness1\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"sort by image uniqueness1 for images with a >0.5 confidence classification\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: find the images most resembling a farm scene\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'skip', 'exclude', 'filter_labels', 'limit', 'select_fields', 'sort_by', 'take', 'exclude_by']\n",
      "Identified brain methods: ['image_similarity', 'text_similarity']\n",
      "Identified brain runs: {'image_similarity': 'clip', 'text_similarity': 'img_sim'}\n",
      "Identified potentially relevant fields: [ground_truth, predictions]\n",
      "Identified label classes: {'ground_truth': [], 'predictions': []}\n",
      "[sort_by_similarity(\"farm scene\", brain_key=\"img_sim\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"find the images most resembling a farm scene\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and error handling\n",
    "- even fully fleshed out, this is not going to be 100% accurate. Need to validate that it is actually creating a valid DataSetView\n",
    "- if it isn't, prompt the user for more specific information\n",
    "    - this could potentially be tailored to which part of the process it failed\n",
    "\n",
    "### Plugin\n",
    "- Everything will be wrapped in a single python function `generate_datasetview_with_chatgpt(dataset, prompt)`\n",
    "- Then need to turn this into a plugin. It will take the `session.dataset` as dataset, and will set `session.view`\n",
    "- This will probably be a menu-item plugin - we could use the ChatGPT symbol - it will take user input\n",
    "- Would love to have a toggle the user can specify for whether they want \n",
    "    - the view created from scratch, or \n",
    "    - the view stages concatenated with their existing view\n",
    "\n",
    "### Future\n",
    "- Memory/chat history\n",
    "- More general question-answering:\n",
    "    - First stage: determine whether the user is asking a question about the entire dataset, or individual samples\n",
    "    - Use ChatGPT as a dispatcher, deciding what other models/processes to invoke.\n",
    "        - if it is about aggregations, then decide what FiftyOne aggregation to perform, and interpret the results\n",
    "        - if it is about a single image, employ BLIPv2 or equivalent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
