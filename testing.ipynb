{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Links\n",
    "- Examples: https://docs.google.com/spreadsheets/d/14pdiODt9PmsD2F___Lpv2rO-RByTtC8PYcEfxL8iIuY/edit#gid=186467173\n",
    "- LangChain: https://github.com/hwchase17/langchain\n",
    "- Chroma: no docker or API_KEY - https://www.trychroma.com/\n",
    "- OpenAI account usage: https://platform.openai.com/account/usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- **Eric & Jacob**: Validation (`validate_stages.py` is not yet working, and has not been incorporated)\n",
    "- **Jacob**: Label class recognition\n",
    "    - determine what classes the user is referring to for each label field\n",
    "- **Allen**: Evaluation Identification Stage:\n",
    "    - `links/evaluation_run_selector.py` \n",
    "    - add stage to `dataset_view_generator.py`\n",
    "    - add `prompts/evaluation_task_rules.txt` prompt\n",
    "    - add examples to Examples spreadsheet in new tab, and then put in `examples` folder\n",
    "- **Leila**: Add support for `hardness` brain runs\n",
    "    - fill out the template in `links/brain_run_selector.py` \n",
    "    - add `prompts/hardness_task_rules.txt` prompt modeled after uniqueness and mistakenness.\n",
    "    - new examples tab `hardness` modeled after `uniqueness` and `mistakenness` tabs and then put in `examples` folder\n",
    "- **Vini**: More examples\n",
    "    - scrape examples from community Slack covering as wide a range of scenarios as possible\n",
    "    - complex filters and view expressions!\n",
    "    - videos\n",
    "    - every type of label\n",
    "    - varied naming conventions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes\n",
    "\n",
    "- Want to validate embedded fields\n",
    "- At present, there is no support for multi-line Python code to generate views. It all needs to be done inline\n",
    "- No support for groups, 3D/point clouds, `concat()`, or `mongo()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install openai langchain chroma`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create an OpenAI account and generate an API key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`export OPENAI_API_KEY=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from links.view_stage_example_selector import generate_view_stage_examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five random images from the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting or creating embeddings for queries...\n",
      "Loading embeddings from file...\n",
      "Saving embeddings to file...\n"
     ]
    }
   ],
   "source": [
    "view_stage_examples_prompt = generate_view_stage_examples_prompt(dataset, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate code to produce the FiftyOne view stages for the following prompts:\n",
      "\n",
      "\n",
      "Input: 10 random images with tables\n",
      "Output: [match(\n",
      "    F(\"ground_truth.detections.label\").contains(\"table\")\n",
      "), take(10)]\n",
      "\n",
      "Input: Take two random samples from the dataset\n",
      "Output: [take(2)]\n",
      "\n",
      "Input: 10 most unique images\n",
      "Output: [sort_by(\"uniqueness\"), limit(10)]\n",
      "\n",
      "Input: show me the 20 most similar imagages to the first image\n",
      "Output: [sort_by_similarity(dataset.first().id, k=20)]\n",
      "\n",
      "Input: sort by similarity to image 10\n",
      "Output: [sort_by_similarity(dataset.skip(9).id)]\n",
      "\n",
      "Input: Images that only contain dogs\n",
      "Output: [match(\n",
      "    F(\"ground_truth.detections.label\").is_subset([\"dog\"])\n",
      ")]\n",
      "\n",
      "Input: take 10 random samples from the dataset with seed=51\n",
      "Output: [take(10, seed=51)]\n",
      "\n",
      "Input: show me 50 random samples\n",
      "Output: [take(50)]\n",
      "\n",
      "Input: show me random samples\n",
      "Output: [shuffle()]\n",
      "\n",
      "Input: exclude the 8th image\n",
      "Output: [exclude([dataset.skip(7).first().id)]\n",
      "\n",
      "Input: Five random images from the dataset\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(view_stage_examples_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate View Stage Descriptions Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.view_stage_description_selector import generate_view_stage_descriptions_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stage_descriptions_prompt = generate_view_stage_descriptions_prompt(view_stage_examples_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some information about view stages you might want to use:\n",
      "\n",
      "\n",
      "    View stage: take\n",
      "    Description: Randomly samples the given number of samples from the collection\n",
      "    Inputs: size: int (non-positive values result in an empty view), seed: optional random seed to use for sample selection\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: match\n",
      "    Description: Filters the samples in the collection by the given filter\n",
      "    Inputs: filter: ViewExpression or MongoDB expression that returns a boolean describing the filter to apply\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: skip\n",
      "    Description: Omits the given number of samples from the head of the collection. Non-positive skip values result in no samples being omitted\n",
      "    Inputs: skip: int\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: sort_by_similarity\n",
      "    Description: Sorts the collection by similarity to a specified query\n",
      "    Inputs: query: ID or iterable of IDs or num_dims vector or num_queries x num_dims array of vectors or text prompt or iterable of text prompts, k: int (defaults to entire collection), reverse: bool (default False), dist_field: name of float field to store distance of each example to query, brain_key: existing fiftyone.brain.compute_similarity() run on the dataset\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: exclude\n",
      "    Description: Exclude samples with specified IDs\n",
      "    Inputs: sample_ids: IDs to exclude. Can be single ID, iterable of IDs, Sample or SampleView instances, iterable of Sample or SampleView instances, or SampleCollection\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: limit\n",
      "    Description: Returns a view with at most the given number of samples\n",
      "    Inputs: limit: maximum number of samples to return. Returns an empty view if limit is non-positive\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: shuffle\n",
      "    Description: Randomly shuffles samples in the collection\n",
      "    Inputs: seed: optional random seed for shuffling\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: sort_by\n",
      "    Description: Sorts samples in the collection by given field(s) or expression(s)\n",
      "    Inputs: field_or_expr: str or ViewExpression or MongoDB aggregation expression or list of (field_or_expr, order) tuples defining a compound sort criteria, where order can be 1 for ascending or any string starting with 'a', or -1 for descending or any string starting with 'd', reverse: bool (default False)\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: exclude_by\n",
      "    Description: Exclude samples with specified field values\n",
      "    Inputs: field: name of the field or embedded field. values: value or iterable of values to exclude by\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: exclude_fields\n",
      "    Description: Exclude specified fields from samples\n",
      "    Inputs: field_names: field name or iterable of field names to exclude, including embedded.field.name\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(view_stage_descriptions_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain method selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.brain_method_selector import select_brain_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueness']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most unique images from the dataset\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueness', 'hardness']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"50 least unique images from the dataset that were hard\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistakenness']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most mistaken images from the dataset\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain run selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.brain_run_selector import select_brain_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniqueness': 'uniqueness1', 'image_similarity': 'img_sim'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"most unique images from the dataset with key 'uniqueness1 that are similar to image 10\"\n",
    "select_brain_runs(dataset, query, [\"uniqueness\", \"image_similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistakenness': 'mistakenness1'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most mistaken images from the dataset with ground truth field 'predictions\"\n",
    "select_brain_runs(dataset, query, [\"mistakenness\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.field_selector import select_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predictions]\n"
     ]
    }
   ],
   "source": [
    "query = \"sort pred1 by number of detections\"\n",
    "fields = select_fields(dataset, query)\n",
    "print(fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label class selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Loading existing dataset 'quickstart'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.label_class_selector import select_label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Bear with bear\n",
      "Class name glove not found for label predictions\n",
      "{'predictions': ['bear', 'glove']}\n"
     ]
    }
   ],
   "source": [
    "query = \"sort Bear and glove predictions by number of detections\"\n",
    "prompt = select_label_classes(dataset, query, \"[predictions]\")\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetView generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_view_generator import get_gpt_view_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: images with tables\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'skip', 'exclude', 'exists', 'filter_labels', 'limit', 'select_fields', 'sort_by', 'take']\n",
      "Identified brain methods: ['text_similarity']\n",
      "Identified brain runs: {'text_similarity': 'clip'}\n",
      "Identified potentially relevant fields: [ground_truth]\n",
      "Class name table not found for label ground_truth\n",
      "Identified label classes: {'ground_truth': ['table']}\n",
      "[match(\n",
      "    F(\"ground_truth.detections.label\").contains(\"table\")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "query = \"images with tables\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: high confidence detections\n",
      "Identified likely view stages: ['filter_labels', 'exists', 'match', 'filter_field', 'limit', 'match_labels', 'sort_by', 'exclude', 'exclude_by', 'exclude_fields']\n",
      "Identified potentially relevant fields: [predictions]\n",
      "Identified label classes: {'predictions': []}\n",
      "[filter_labels(\"ground_truth\", F(\"confidence\") > 0.95)]\n"
     ]
    }
   ],
   "source": [
    "query = \"high confidence detections\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: first 30 images with a ground truth detection\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'exists', 'filter_field', 'filter_labels', 'limit_labels', 'select_fields', 'skip', 'sort_by', 'take']\n",
      "Identified potentially relevant fields: [ground_truth]\n",
      "Identified label classes: {'ground_truth': []}\n",
      "[match(F(\"ground_truth.detections\").length() > 0), take(30)]\n"
     ]
    }
   ],
   "source": [
    "query = \"first 30 images with a ground truth detection\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: sort by image uniqueness1 for images with a >0.5 confidence classification\n",
      "Identified likely view stages: ['exists', 'match', 'sort_by', 'sort_by_similarity', 'filter_labels', 'filter_keypoints', 'match_labels', 'skip', 'exclude', 'exclude_by']\n",
      "Identified brain methods: ['uniqueness']\n",
      "Identified brain runs: {'uniqueness': 'uniqueness1'}\n",
      "Identified potentially relevant fields: [predictions, mistakenness1_eval_fp]\n",
      "Identified label classes: {'predictions': []}\n",
      "[match_labels(\n",
      "    filter=F(\"predictions.confidence\") > 0.5,\n",
      "    fields=\"ground_truth\"\n",
      "), sort_by(\"uniqueness1\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"sort by image uniqueness1 for images with a >0.5 confidence classification\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: find the images most resembling a farm scene\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'skip', 'exclude', 'filter_labels', 'limit', 'select_fields', 'sort_by', 'take', 'exclude_by']\n",
      "Identified brain methods: ['image_similarity', 'text_similarity']\n",
      "Identified brain runs: {'image_similarity': 'clip', 'text_similarity': 'img_sim'}\n",
      "Identified potentially relevant fields: [ground_truth, predictions]\n",
      "Identified label classes: {'ground_truth': [], 'predictions': []}\n",
      "[sort_by_similarity(\"farm scene\", brain_key=\"img_sim\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"find the images most resembling a farm scene\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and error handling\n",
    "- even fully fleshed out, this is not going to be 100% accurate. Need to validate that it is actually creating a valid DataSetView\n",
    "- if it isn't, prompt the user for more specific information\n",
    "    - this could potentially be tailored to which part of the process it failed\n",
    "\n",
    "### Plugin\n",
    "- Everything will be wrapped in a single python function `generate_datasetview_with_chatgpt(dataset, prompt)`\n",
    "- Then need to turn this into a plugin. It will take the `session.dataset` as dataset, and will set `session.view`\n",
    "- This will probably be a menu-item plugin - we could use the ChatGPT symbol - it will take user input\n",
    "- Would love to have a toggle the user can specify for whether they want \n",
    "    - the view created from scratch, or \n",
    "    - the view stages concatenated with their existing view\n",
    "\n",
    "### Future\n",
    "- Memory/chat history\n",
    "- More general question-answering:\n",
    "    - First stage: determine whether the user is asking a question about the entire dataset, or individual samples\n",
    "    - Use ChatGPT as a dispatcher, deciding what other models/processes to invoke.\n",
    "        - if it is about aggregations, then decide what FiftyOne aggregation to perform, and interpret the results\n",
    "        - if it is about a single image, employ BLIPv2 or equivalent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
