{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Links\n",
    "- Examples: https://docs.google.com/spreadsheets/d/14pdiODt9PmsD2F___Lpv2rO-RByTtC8PYcEfxL8iIuY/edit#gid=186467173\n",
    "- LangChain: https://github.com/hwchase17/langchain\n",
    "- Chroma: no docker or API_KEY - https://www.trychroma.com/\n",
    "- OpenAI account usage: https://platform.openai.com/account/usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- **Eric & Jacob**: Validation (`validate_stages.py` is not yet working, and has not been incorporated)\n",
    "- **Jacob**: Label class recognition\n",
    "    - determine what classes the user is referring to for each label field\n",
    "- **Allen**: Evaluation Identification Stage:\n",
    "    - `links/evaluation_run_selector.py` \n",
    "    - add stage to `dataset_view_generator.py`\n",
    "    - add `prompts/evaluation_task_rules.txt` prompt\n",
    "    - add examples to Examples spreadsheet in new tab, and then put in `examples` folder\n",
    "- **Leila**: Add support for `hardness` brain runs\n",
    "    - fill out the template in `links/brain_run_selector.py` \n",
    "    - add `prompts/hardness_task_rules.txt` prompt modeled after uniqueness and mistakenness.\n",
    "    - new examples tab `hardness` modeled after `uniqueness` and `mistakenness` tabs and then put in `examples` folder\n",
    "- **Vini**: More examples\n",
    "    - scrape examples from community Slack covering as wide a range of scenarios as possible\n",
    "    - complex filters and view expressions!\n",
    "    - videos\n",
    "    - every type of label\n",
    "    - varied naming conventions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes\n",
    "\n",
    "- Want to validate embedded fields\n",
    "- At present, there is no support for multi-line Python code to generate views. It all needs to be done inline\n",
    "- No support for groups, 3D/point clouds, `concat()`, or `mongo()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install openai langchain chroma`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create an OpenAI account and generate an API key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`export OPENAI_API_KEY=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.view_stage_example_selector import generate_view_stage_examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five random images from the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stage_examples_prompt = generate_view_stage_examples_prompt(dataset, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(view_stage_examples_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate View Stage Descriptions Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.view_stage_description_selector import generate_view_stage_descriptions_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stage_descriptions_prompt = generate_view_stage_descriptions_prompt(view_stage_examples_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(view_stage_descriptions_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.algorithm_selector import select_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueness']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most unique images from the dataset\"\n",
    "select_algorithms(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueness', 'hardness']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"50 least unique images from the dataset that were hard\"\n",
    "select_algorithms(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistakenness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most mistaken images from the dataset\"\n",
    "select_algorithms(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evaluation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"false positive predictions\"\n",
    "select_algorithms(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.run_selector import select_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Loading existing dataset 'quickstart'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniqueness': 'uniqueness1', 'image_similarity': 'img_sim'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"most unique images from the dataset with key 'uniqueness1 that are similar to image 10\"\n",
    "select_runs(dataset, query, [\"uniqueness\", \"image_similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistakenness': 'mistakenness1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most mistaken images from the dataset with ground truth field 'predictions\"\n",
    "select_runs(dataset, query, [\"mistakenness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hardness': 'hardness'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most challenging images from dataset\"\n",
    "select_runs(dataset, query, [\"hardness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Difficulty from key 'test_hardness'\"\n",
    "select_runs(dataset, query, [\"hardness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'key': 'eval_cls',\n",
       "  'method': 'simple',\n",
       "  'pred_field': 'cls_pd',\n",
       "  'gt_field': 'cls_gt'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"true positives\"\n",
    "select_runs(dataset, query, [\"evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metadata found. To compute metadata for your samples, please run the following command:\n",
      "\n",
      "        ```\n",
      "        dataset.compute_metadata()\n",
      "        ```\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"images with largest height\"\n",
    "select_runs(dataset, query, [\"metadata\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.field_selector import select_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sort pred1 by number of detections\"\n",
    "fields = select_fields(dataset, query)\n",
    "print(fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label class selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.label_class_selector import select_label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sort Bear and glove predictions by number of detections\"\n",
    "prompt = select_label_classes(dataset, query, \"[predictions]\")\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetView generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from gpt_view_generator import get_gpt_view_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: images with tables\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'skip', 'exclude', 'exists', 'filter_labels', 'limit', 'select_fields', 'sort_by', 'take']\n",
      "Identified algorithms: ['text_similarity']\n",
      "No similarity index found that supports text prompts. To generate a similarity index for your samples, please run the following command:\n",
      "\n",
      "        ```\n",
      "        import fiftyone.brain as fob\n",
      "        fob.compute_similarity(\n",
      "            dataset, \n",
      "            model='clip-vit-base32-torch',\n",
      "            brain_run_key='text_sim',\n",
      "            )\n",
      "        ```\n",
      "        \n",
      "Identified potentially relevant fields: [ground_truth]\n",
      "Class name table not found for label ground_truth\n",
      "Identified label classes: {'ground_truth': ['table']}\n",
      "[match(\n",
      "    F(\"ground_truth.detections.label\").contains(\"table\")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "query = \"images with tables\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"high confidence detections\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"first 30 images with a ground truth detection\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sort by image uniqueness1 for images with a >0.5 confidence classification\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: find the images most resembling a farm scene\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'skip', 'exclude', 'filter_labels', 'limit', 'select_fields', 'sort_by', 'take', 'exclude_by']\n",
      "Identified algorithms: ['image_similarity']\n",
      "Identified runs: {'image_similarity': {'key': 'clip', 'method': 'sklearn', 'embeddings_field': None, 'model': 'clip-vit-base32-torch', 'patches_field': None}}\n",
      "Identified potentially relevant fields: [ground_truth, predictions]\n",
      "Class name farm not found for label ground_truth\n",
      "Identified label classes: {'ground_truth': ['farm'], 'predictions': []}\n",
      "[sort_by_similarity(\"farm\", brain_key=\"clip\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"find the images most resembling a farm scene\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: get true positives with eval_det_iou08_tp\n",
      "Identified likely view stages: ['exists', 'filter_labels', 'match', 'filter_field', 'filter_keypoints', 'match_labels', 'sort_by', 'exclude', 'exclude_by', 'exclude_fields']\n",
      "Identified algorithms: ['evaluation']\n",
      "Identified runs: {'evaluation': {'key': 'eval_cls', 'method': 'simple', 'pred_field': 'cls_pd', 'gt_field': 'cls_gt'}}\n",
      "Identified potentially relevant fields: [eval_det_iou08_tp]\n",
      "[filter_labels(\n",
      "    \"eval_det_iou08_tp\",\n",
      "    F(\"eval_det_iou08_tp\") == \"tp\"\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "query = \"get true positives with eval_det_iou08_tp\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: smallest 20 images\n",
      "Getting or creating embeddings for queries...\n",
      "Loading embeddings from file...\n",
      "Saving embeddings to file...\n",
      "Identified likely view stages: ['match', 'limit', 'sort_by_similarity', 'skip', 'exclude', 'sort_by', 'take', 'exclude_by', 'exclude_fields', 'exclude_frames']\n",
      "Identified algorithms: ['metadata']\n",
      "No metadata found. To compute metadata for your samples, please run the following command:\n",
      "\n",
      "        ```\n",
      "        dataset.compute_metadata()\n",
      "        ```\n",
      "        \n",
      "field_names ['id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions']\n",
      "available_fields ['id: string', 'filepath: string', 'tags: list']\n",
      "fs ['id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions']\n",
      "fn ground_truth\n",
      "fn uniqueness\n",
      "fn predictions\n",
      "Identified potentially relevant fields: [filepath]\n",
      "[sort_by(\"metadata.size_bytes\"), limit(20)]\n"
     ]
    }
   ],
   "source": [
    "query = \"smallest 20 images\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and error handling\n",
    "- even fully fleshed out, this is not going to be 100% accurate. Need to validate that it is actually creating a valid DataSetView\n",
    "- if it isn't, prompt the user for more specific information\n",
    "    - this could potentially be tailored to which part of the process it failed\n",
    "\n",
    "### Plugin\n",
    "- Everything will be wrapped in a single python function `generate_datasetview_with_chatgpt(dataset, prompt)`\n",
    "- Then need to turn this into a plugin. It will take the `session.dataset` as dataset, and will set `session.view`\n",
    "- This will probably be a menu-item plugin - we could use the ChatGPT symbol - it will take user input\n",
    "- Would love to have a toggle the user can specify for whether they want \n",
    "    - the view created from scratch, or \n",
    "    - the view stages concatenated with their existing view\n",
    "\n",
    "### Future\n",
    "- Memory/chat history\n",
    "- More general question-answering:\n",
    "    - First stage: determine whether the user is asking a question about the entire dataset, or individual samples\n",
    "    - Use ChatGPT as a dispatcher, deciding what other models/processes to invoke.\n",
    "        - if it is about aggregations, then decide what FiftyOne aggregation to perform, and interpret the results\n",
    "        - if it is about a single image, employ BLIPv2 or equivalent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
