query,available_evaluations,selected_evaluation
"find missed detections in field 'model1_preds'","[(key: eval_det, config_method: coco, config_pred_field: model2_preds, config_gt_field: ground_truth, config_iou: 0.5), (key: eval_det_iou08, config_method: coco, config_pred_field: model1_preds, config_gt_field: ground_truth, config_iou: 0.8)]",eval_det_iou08
"show me all samples where the correct label is not in the top 5 predicted classes","[(key: eval_cls2, config_method: simple, config_pred_field: cls_pd2, config_gt_field: cls_gt), (key: eval_cls2_topk, config_method: top-k, config_pred_field: cls_pd2, config_gt_field: cls_gt)]",eval_cls2_topk
"show me all misclassified examples","[(key: eval_cls2, config_method: simple, config_pred_field: predictions, config_gt_field: ground_truth), (key: eval_cls2_topk, config_method: top-k, config_pred_field: predictions, config_gt_field: ground_truth)]",eval_cls2
"give me all misclassifications under the evaluation run 'eval20230204'","[(key: eval20230204, config_method: simple, config_pred_field: cls_pd, config_gt_field: cls_gt), (key: eval20230202, config_method: simple, config_pred_field: cls_pd2, config_gt_field: cls_gt)]",eval20230204
"find high confidence false positives under the coco-style evaluation","[(key: eval_det_iou08, config_method: coco, config_pred_field: predictions, config_gt_field: ground_truth, config_iou: 0.8), (key: eval_det_oi, config_method: open-images, config_pred_field: predictions, config_gt_field: ground_truth, config_iou: 0.5)]",eval_det_iou08
"what are incorrect predictions when the iou threshold is 0.8","[(key: eval_20221010, config_method: coco, config_pred_field: predictions, config_gt_field: ground_truth, config_iou: 0.8), (key: eval20221015, config_method: open-images, config_pred_field: predictions, config_gt_field: ground_truth, config_iou: 0.5)]",eval_20221010
