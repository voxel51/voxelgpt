{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "from links.evaluation_selector import EvaluationYesNo\n",
    "from links.evaluation_selector import DefaultEvaluationSelector\n",
    "import links.evaluation_selector as les"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1\n",
    "\n",
    "dsname='ds_chat_2detpreds'\n",
    "ds = fo.load_dataset(dsname)\n",
    "des = DefaultEvaluationSelector(ds)\n",
    "\n",
    "qry = \"find missed detections in 'new_preds'\"\n",
    "evals1, prompt1, resp1 = des.llm_select_evaluation(qry)\n",
    "#print(prompt)\n",
    "print(qry, ':', resp1)\n",
    "\n",
    "# test2\n",
    "dsname='ds_chat_2detGTs'\n",
    "ds = fo.load_dataset(dsname)\n",
    "des = DefaultEvaluationSelector(ds)\n",
    "\n",
    "qry = \"find missed detections against ground truth 'gt_new'\"\n",
    "evals2, prompt2, resp2 = des.llm_select_evaluation(qry)\n",
    "#print(prompt)\n",
    "print(qry, ':', resp2)\n",
    "\n",
    "# test3\n",
    "dsname='ds_chat_2detIOUs'\n",
    "ds = fo.load_dataset(dsname)\n",
    "des = DefaultEvaluationSelector(ds)\n",
    "\n",
    "qry = \"find missed detections using the greater  IOU threshold\"\n",
    "evals3, prompt3, resp3 = des.llm_select_evaluation(qry)\n",
    "#print(prompt)\n",
    "print(qry, ':', resp3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Y/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyn = EvaluationYesNo()\n",
    "\n",
    "qry = 'All samples that contain cars'\n",
    "resp, prompt = eyn.generate_evaluation_yesno(qry)\n",
    "print(qry)\n",
    "#print(prompt)\n",
    "print(resp)\n",
    "\n",
    "qry = 'All images with dogs but not cats'\n",
    "resp, prompt = eyn.generate_evaluation_yesno(qry)\n",
    "print(qry)\n",
    "#print(prompt)\n",
    "print(resp)\n",
    "\n",
    "qry = 'All high confidence false positives'\n",
    "resp, prompt = eyn.generate_evaluation_yesno(qry)\n",
    "print(qry)\n",
    "#print(prompt)\n",
    "print(resp)\n",
    "\n",
    "qry = 'All samples with at least 5 detections'\n",
    "resp, prompt = eyn.generate_evaluation_yesno(qry)\n",
    "print(qry)\n",
    "#print(prompt)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = 'All samples with a car in the top half of the image'\n",
    "resp, prompt = eyn.generate_evaluation_yesno(qry)\n",
    "print(qry)\n",
    "#print(prompt)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = 'All missed detections in the \"preds\" field'\n",
    "resp, prompt = eyn.generate_evaluation_yesno(qry)\n",
    "print(qry)\n",
    "#print(prompt)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
