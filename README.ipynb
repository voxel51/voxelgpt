{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Links\n",
    "- Examples: https://docs.google.com/spreadsheets/d/14pdiODt9PmsD2F___Lpv2rO-RByTtC8PYcEfxL8iIuY/edit#gid=186467173\n",
    "- LangChain: https://github.com/hwchase17/langchain\n",
    "- Chroma: no docker or API_KEY - https://www.trychroma.com/\n",
    "- OpenAI account usage: https://platform.openai.com/account/usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- **Eric & Jacob**: Validation (`validate_stages.py` is not yet working, and has not been incorporated)\n",
    "- **Jacob**: Label class recognition\n",
    "    - determine what classes the user is referring to for each label field\n",
    "- **Allen**: Evaluation Identification Stage:\n",
    "    - `links/evaluation_run_selector.py` \n",
    "    - add stage to `dataset_view_generator.py`\n",
    "    - add `prompts/evaluation_task_rules.txt` prompt\n",
    "    - add examples to Examples spreadsheet in new tab, and then put in `examples` folder\n",
    "- **Leila**: Add support for `hardness` brain runs\n",
    "    - fill out the template in `links/brain_run_selector.py` \n",
    "    - add `prompts/hardness_task_rules.txt` prompt modeled after uniqueness and mistakenness.\n",
    "    - new examples tab `hardness` modeled after `uniqueness` and `mistakenness` tabs and then put in `examples` folder\n",
    "- **Vini**: More examples\n",
    "    - scrape examples from community Slack covering as wide a range of scenarios as possible\n",
    "    - complex filters and view expressions!\n",
    "    - videos\n",
    "    - every type of label\n",
    "    - varied naming conventions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes\n",
    "\n",
    "- Want to validate embedded fields\n",
    "- At present, there is no support for multi-line Python code to generate views. It all needs to be done inline\n",
    "- No support for groups, 3D/point clouds, `concat()`, or `mongo()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install openai langchain chroma`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create an OpenAI account and generate an API key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`export OPENAI_API_KEY=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.view_stage_example_selector import generate_view_stage_examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Five random images from the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "view_stage_examples_prompt = generate_view_stage_examples_prompt(dataset, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate code to produce the FiftyOne view stages for the following prompt:\n",
      "\n",
      "\n",
      "Input: show me 50 random samples\n",
      "Output: dataset.take(50)\n",
      "\n",
      "Input: Take two random samples from the dataset\n",
      "Output: dataset.take(2)\n",
      "\n",
      "Input: 10 most unique images\n",
      "Output: dataset.filter_labels(\"predictions\", F(\"confidence\") > 0.95)\n",
      "\n",
      "Input: 10 random images with tables\n",
      "Output: dataset.match(\n",
      "    F(\"ground_truth.detections.label\").contains(\"table\")\n",
      ").take(10)\n",
      "\n",
      "Input: show me random samples\n",
      "Output: dataset.shuffle()\n",
      "\n",
      "Input: skip the first 10 samples\n",
      "Output: dataset.skip(10)\n",
      "\n",
      "Input: first 100 samples\n",
      "Output: dataset.limit(100)\n",
      "\n",
      "Input: Don't show me the fifth and sixth samples\n",
      "Output: ids = [dataset.skip(4).first().id, dataset.skip(4).first().id];\n",
      "dataset.exclude(ids)\n",
      "\n",
      "Input: Images that only contain dogs\n",
      "Output: dataset.match(\n",
      "    F(\"ground_truth.detections.label\").is_subset([\"dog\"])\n",
      ")\n",
      "\n",
      "Input: ten least wrong predictions\n",
      "Output: dataset.sort_by(\"mistakenness\", reverse=False)[:10]\n",
      "\n",
      "Input: Five random images from the dataset\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(view_stage_examples_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate View Stage Descriptions Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.view_stage_description_selector import generate_view_stage_descriptions_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stage_descriptions_prompt = generate_view_stage_descriptions_prompt(view_stage_examples_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some information about view stages you might want to use:\n",
      "\n",
      "\n",
      "    View stage: skip\n",
      "    Description: Omits the given number of samples from the head of the collection. Non-positive skip values result in no samples being omitted\n",
      "    Inputs: skip: int\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: take\n",
      "    Description: Randomly samples the given number of samples from the collection\n",
      "    Inputs: size: int (non-positive values result in an empty view), seed: optional random seed to use for sample selection\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: match\n",
      "    Description: Filters the samples in the collection by the given filter\n",
      "    Inputs: filter: ViewExpression or MongoDB expression that returns a boolean describing the filter to apply\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: exclude\n",
      "    Description: Exclude samples with specified IDs\n",
      "    Inputs: sample_ids: IDs to exclude. Can be single ID, iterable of IDs, Sample or SampleView instances, iterable of Sample or SampleView instances, or SampleCollection\n",
      "\n",
      "    \n",
      "\n",
      "    View stage: filter_labels\n",
      "    Description: Filters Label field of each sample. For single Label fields, fields for which filter returns False are replaced with None. For Label list fields, label elements for which filter returns False are omitted from view\n",
      "    Inputs: field: label field to filter. filter: ViewExpression or MongoDB expression returning boolean describing filter to apply. only_matches (True): whether to only include samples with at least one label after filtering or include all samples. trajectories: whether to match entire object trajectories for which object matches given filter on at least one frame. Only applicable to datasets with videos and frame-level label fields whose objects have their index attributes populated\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(view_stage_descriptions_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain method selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.brain_method_selector import select_brain_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueness']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most unique images from the dataset\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueness', 'hardness']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"50 least unique images from the dataset that were hard\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistakenness']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most mistaken images from the dataset\"\n",
    "select_brain_methods(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain run selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.brain_run_selector import select_brain_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniqueness': 'uniqueness1', 'image_similarity': 'img_sim'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"most unique images from the dataset with key 'uniqueness1 that are similar to image 10\"\n",
    "select_brain_runs(dataset, query, [\"uniqueness\", \"image_similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistakenness': 'mistakenness'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Five most mistaken images from the dataset with ground truth field 'predictions\"\n",
    "select_brain_runs(dataset, query, [\"mistakenness\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from links.field_selector import select_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[id: string, filepath: string, tags: list, ground_truth: Detections, predictions: Detections, classif: Classification, mistakenness1_eval_fp: int, pred1: Detections]\n",
      "[pred1]\n"
     ]
    }
   ],
   "source": [
    "query = \"sort pred1 by number of detections\"\n",
    "fields = select_fields(dataset, query)\n",
    "print(fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetView generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_view_generator import get_gpt_view_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: images with tables\n",
      "Identified likely view stages: ['sort_by_similarity', 'limit', 'match', 'skip', 'exclude']\n",
      "Identified brain methods: ['text_similarity']\n",
      "Identified brain runs: {'text_similarity': 'qdrant_gt_patches'}\n",
      "Identified fields: [ground_truth, predictions]\n",
      "[match(\n",
      "    F(\"ground_truth.detections.label\").contains(\"table\")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "query = \"images with tables\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: patches of rabbits with high confidence\n",
      "Identified likely view stages: ['filter_labels', 'exists', 'sort_by_similarity', 'match', 'select_fields']\n",
      "Identified brain methods: ['mistakenness']\n",
      "Identified brain runs: {'mistakenness': 'mistakenness5'}\n",
      "Identified fields: [predictions]\n",
      "[filter_labels(\"predictions\", F(\"label\") == \"rabbit\", F(\"confidence\") > 0.95)]\n"
     ]
    }
   ],
   "source": [
    "query = \"patches of rabbits with high confidence\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar examples for query: first 30 images with a ground truth detection\n",
      "Identified likely view stages: ['match', 'sort_by_similarity', 'limit', 'limit_labels', 'skip']\n",
      "Identified brain methods: []\n",
      "Identified brain runs: {}\n",
      "Identified fields: [ground_truth]\n",
      "[match(F(\"ground_truth.detections\").length() > 0), limit(30)]\n"
     ]
    }
   ],
   "source": [
    "query = \"first 30 images with a ground truth detection\"\n",
    "view_text = get_gpt_view_text(dataset, query)\n",
    "print(view_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and error handling\n",
    "- even fully fleshed out, this is not going to be 100% accurate. Need to validate that it is actually creating a valid DataSetView\n",
    "- if it isn't, prompt the user for more specific information\n",
    "    - this could potentially be tailored to which part of the process it failed\n",
    "\n",
    "### Plugin\n",
    "- Everything will be wrapped in a single python function `generate_datasetview_with_chatgpt(dataset, prompt)`\n",
    "- Then need to turn this into a plugin. It will take the `session.dataset` as dataset, and will set `session.view`\n",
    "- This will probably be a menu-item plugin - we could use the ChatGPT symbol - it will take user input\n",
    "- Would love to have a toggle the user can specify for whether they want \n",
    "    - the view created from scratch, or \n",
    "    - the view stages concatenated with their existing view\n",
    "\n",
    "### Future\n",
    "- Memory/chat history\n",
    "- More general question-answering:\n",
    "    - First stage: determine whether the user is asking a question about the entire dataset, or individual samples\n",
    "    - Use ChatGPT as a dispatcher, deciding what other models/processes to invoke.\n",
    "        - if it is about aggregations, then decide what FiftyOne aggregation to perform, and interpret the results\n",
    "        - if it is about a single image, employ BLIPv2 or equivalent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
