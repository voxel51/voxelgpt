In the computer vision library FiftyOne, a field is an attribute of a sample that stores information about the sample. Fields can be dynamically created, modified, and deleted from samples on a per-sample basis. When a new Field is assigned to a Sample in a Dataset, it is automatically added to the datasetâ€™s schema and thus accessible on all other samples in the dataset. If a field exists on a dataset but has not been set on a particular sample, its value will be None.

By default, all Sample instances have the following fields:
- id (string): The ID of the sample in its parent dataset, which is generated automatically when the sample is added to a dataset, or None if the sample does not belong to a dataset
- filepath (string): The path to the source data on disk. Must be provided at sample creation time
- tags (list of strings): A list of user-defined tags that can be used to organize samples. It can be used like a standard Python list:

A Field can be any primitive type, such as bool, int, float, str, date, datetime, list, dict, or more complex data structures like label types. FiftyOne also supports custom fields, which are fields that are defined by the user and can be used to store arbitrary data.

Fields must have the same type (or None) across all samples in the dataset. Setting a field to an inappropriate type raises an error

You can also declare nested fields on existing embedded documents using dot notation:
```
# Declare a new attribute on a `Classification` field
dataset.add_sample_field("predictions.breed", fo.StringField)
```

List fields: If your dataset contains a ListField with no value type declared, you can add the type later by appending [] to the field path:
```
field = dataset.get_field("more_tags")
print(field.field)  # None

# Declare the subfield types of an existing untyped list field
dataset.add_sample_field("more_tags[]", fo.StringField)

field = dataset.get_field("more_tags")
print(field.field)  # StringField

# List fields can also contain embedded documents
dataset.add_sample_field(
    "info[]",
    fo.EmbeddedDocumentField,
    embedded_doc_type=fo.DynamicEmbeddedDocument,
)

field = dataset.get_field("info")
print(field.field)  # EmbeddedDocumentField
print(field.field.document_type)  # DynamicEmbeddedDocument
```

Metadata field: Some Sample instances have a metadata field, which can optionally be populated with a Metadata instance that stores data type-specific metadata about the raw data in the sample. For image samples, the ImageMetadata class is used to store information about images, including their size_bytes, mime_type, width, height, and num_channels.

Datetime fields: Internally, FiftyOne stores all dates as UTC timestamps, but you can provide any valid datetime object when setting a DateTimeField of a sample, including timezone-aware datetimes, which are internally converted to UTC format for safekeeping. When you access a datetime field of a sample in a dataset, it is retrieved as a naive datetime instance expressed in UTC format.

Label fields: The Label class hierarchy is used to store semantic information about ground truth or predicted labels in a sample. These are the available label types:

    Classification: label - a string indicating the label;confidence - a float in the range [0, 1] indicating the confidence of the classification;logits - a list of floats representing the logits associated with the labels,
    Classifications: a list of Classification instances,
    Detection: label - a string indicating the label;bounding_box - a list of four floats in the range [0, 1] indicating the top-left corner coordinates, width, and height of the bounding box;mask - a 2D binary or 0/1 integer numpy array representing an instance segmentation mask for the detection within its bounding box;confidence - a float in the range [0, 1] indicating the confidence of the detection;index - an integer indicating the index for the object;attributes - a dictionary mapping attribute names to Attribute instances,
    Detections: a list of Detection instances,
    GeoLocation: point - a list of two floats representing a [longitude, latitude] point;line - a list of lists, with each inner list containing two floats representing a [longitude, latitude] coordinate pair;polygon - a list of lists of lists, with the first outer list describing the boundary of the polygon and any remaining entries describing holes. Each inner list contains two floats representing a [longitude, latitude] coordinate pair,
    GeoLocations: points - a list of points;lines - a list of lines;polygons - a list of polygons,
    Heatmap: map - a 2D numpy array representing a heatmap;map_path - a string representing the path to the heatmap image on disk;range - an optional list of two floats indicating the minimum and maximum values of the map. If None, the range will be automatically determined based on the data type of the map,
    Keypoint: label - a string indicating the label for the keypoints;points - a list of (x, y) keypoints in the range [0, 1];confidence - a list of floats in the range [0, 1] indicating the confidence of each keypoint;index - an integer indicating the index for the keypoints;attributes - a dictionary mapping attribute names to Attribute instances,
    Keypoints: a list of Keypoint instances,
    Polyline: label - a string indicating the label for the polyline;points - a list of lists of (x, y) points in the range [0, 1] describing the vertices of each shape in the polyline;confidence - a float in the range [0, 1] indicating the confidence of the polyline;index - an integer indicating the index for the polyline;closed - a boolean indicating whether the shapes are closed, i.e., an edge should be drawn from the last vertex to the first vertex of each shape;filled - a boolean indicating whether the polyline represents polygons, i.e., shapes that should be filled when rendering them;attributes - a dictionary mapping attribute names to Attribute instances for the polyline,
    Polylines: polylines - a list of Polylines instances,
    Regression: value - the regression value;confidence - a confidence in [0, 1] for the regression,
    Segmentation: mask - a numpy array with integer values encoding the semantic labels;mask_path - the path to the segmentation image on disk,
    TemporalDetection: label - the label string;support - the [first, last] frame numbers, inclusive;confidence - a confidence in [0, 1] for the detection,
    TemporalDetections: detections - a list of TemporalDetection instance"

Your task is to determine the names of the fields that are required to convert a given natural language query into a DatasetView.

Here are the rules:
- You must respond to the natural language query with a list of strings.
- The list must contain the names of the fields that are required to convert the query into a DatasetView.
- Each of the strings in the list must be a valid field name in the dataset, i.e., it must be a valid key in the dataset's schema. The strings must exactly match.
- Just return the final list of strings, no intermediate code snippets or explanation, or newlines.
- If no fields are required, return an empty list.
- If you are unsure of the answer, just give me your best guess. You must respond with a list of strings.