"""
VoxelGPT entrypoints.

| Copyright 2017-2023, Voxel51, Inc.
| `voxel51.com <https://voxel51.com/>`_
|
"""
from collections import defaultdict
import re
import sys

import fiftyone as fo

from links.query_moderator import moderate_query
from links.dataset_schema_handler import query_schema
from links.query_intent_classifier import classify_query_intent
from links.docs_query_dispatcher import run_docs_query, stream_docs_query
from links.computer_vision_query_dispatcher import (
    run_computer_vision_query,
    stream_computer_vision_query,
)
from links.view_stage_example_selector import (
    generate_view_stage_examples_prompt,
)
from links.view_stage_description_selector import (
    generate_view_stage_descriptions_prompt,
    get_most_relevant_view_stages,
)
from links.algorithm_selector import select_algorithms
from links.run_selector import select_runs
from links.field_selector import select_fields
from links.label_class_selector import select_label_classes
from links.dataset_view_generator import get_gpt_view_stage_strings
from links.effective_query_generator import generate_effective_query


_SUPPORTED_DIALECTS = ("string", "markdown", "raw")


def ask_voxelgpt_interactive(
    sample_collection=None,
    session=None,
    chat_history=None,
):
    """Launches an interactive session with VoxelGPT.

    You will be prompted by ``input()`` to provide queries, any responses from
    VoxelGPT will be printed to stdout, and any views created are automatically
    loaded in the App.

    If you provide a chat history, your query and VoxelGPT's responses will be
    added to it.

    Special keywords:

    -   Type `help` to see a help message
    -   Type `reset` to clear your chat history
    -   Type `exit` or `^c` to end your session

    Args:
        sample_collection (None): a
            :class:`fiftyone.core.collections.SampleCollection` to query
        session (None): an optional :class:`fiftyone.core.session.Session` to
            load views in. By default, a new App session is launched
        chat_history (None): an optional chat history list
    """
    if chat_history is None:
        chat_history = []

    empty = 0

    while True:
        if empty >= 5:
            query = input("How can I help you? (try 'help' or 'exit') ")
        else:
            query = input("How can I help you? ")

        if not query:
            empty += 1
            continue

        if query.strip().lower() == "exit":
            break

        if query.strip().lower() == "reset":
            chat_history.clear()
            continue

        empty = 0

        coll = ask_voxelgpt(
            query,
            sample_collection=sample_collection,
            chat_history=chat_history,
        )

        if coll is None:
            continue

        if session is None:
            session = fo.launch_app(sample_collection, auto=False)

        if session._collection != coll:
            if isinstance(coll, fo.Dataset):
                session.dataset = coll
            elif isinstance(coll, fo.DatasetView):
                session.view = coll


def ask_voxelgpt(
    query,
    sample_collection=None,
    allow_streaming=True,
    chat_history=None,
):
    """Prompts VoxelGPT with the given query with respect to the given sample
    collection.

    If your query is understood as a view to load, it will be returned.

    If you provide a chat history, your query and VoxelGPT's responses will be
    added to it.

    Args:
        query: a prompt string
        sample_collection (None): a
            :class:`fiftyone.core.collections.SampleCollection` to query
        allow_streaming (True): whether to allow streaming responses
        chat_history (None): an optional chat history list

    Returns:
        a :class:`fiftyone.core.view.DatasetView`, or None if the query did not
        result in a view creation
    """
    view = None

    for response in ask_voxelgpt_generator(
        query,
        sample_collection=sample_collection,
        dialect="string",
        allow_streaming=allow_streaming,
        chat_history=chat_history,
    ):
        type = response["type"]
        data = response["data"]

        if type == "view":
            view = data["view"]
        elif type == "message":
            if not data["overwrite"]:
                print(data["message"])
        elif type == "streaming":
            sys.stdout.write(data["content"])
            if data["last"]:
                sys.stdout.write("\n")

            sys.stdout.flush()

    return view


def ask_voxelgpt_generator(
    query,
    sample_collection=None,
    dialect="string",
    allow_streaming=True,
    chat_history=None,
):
    """Generator that emits responses from VoxelGPT with respect to the given
    query.

    The generator may emit the following types of content:

    -   Messages in the format::

        {
            "type": "message",
            "data": {
                "message": message,         # in your chosen dialect
                "history": history,         # string added to `chat_history`
                "overwrite": True/False     # overwrite previous message?
            }
        }

    -   Streaming content in the format:

        {
            "type": "streaming",
            "data": {
                "content": content,         # a chunk of streaming content
                "last": True/False          # last chunk in the stream?
            }
        }

    -   Views in the format::

        {
            "type": "view",
            "data": {
                "view": view
            }
        }

    -   Warnings in the formatt::

        {
            "type": "warning",
            "data": {
                "message": message
            }
        }

    You can use the ``dialect`` parameter to configure the message format.

    If you provide a chat history, your query and VoxelGPT's responses will be
    added to it.

    Args:
        query: a prompt string
        sample_collection (None): a
            :class:`fiftyone.core.collections.SampleCollection` to query
        dialect ("string"): the response format to return. Supported values are
            ``("string", "markdown", "raw")``
        allow_streaming (True): whether to allow streaming responses
        chat_history (None): an optional chat history list
    """
    if dialect not in _SUPPORTED_DIALECTS:
        raise ValueError(
            f"Unsupported dialect '{dialect}'. Supported: {_SUPPORTED_DIALECTS}"
        )

    if chat_history is None:
        chat_history = []

    def _respond(message, overwrite=False):
        if isinstance(message, str):
            message = {"string": message, "markdown": message}

        str_msg = message.get("string", None)
        if str_msg is not None:
            _log_chat_history("GPT", str_msg, chat_history)

        if dialect == "raw":
            return str_msg

        msg = message.get(dialect, None)
        if msg is not None:
            return _emit_message(msg, str_msg, overwrite=overwrite)

    if not moderate_query(query):
        yield _respond(_moderation_fail_message())
        return

    if query.strip().lower() == "help":
        yield _respond(_help_message())
        return

    _log_chat_history("User", query, chat_history)

    # Generate a new query that incorporates the chat history
    if chat_history:
        query = generate_effective_query(chat_history)

    # Handle schema queries first
    schema_response = query_schema(query, sample_collection)
    if schema_response:
        yield _respond(schema_response)
        return

    # Intent classification
    intent = classify_query_intent(query)
    if intent == "documentation":
        if allow_streaming:
            message = ""
            for content in stream_docs_query(query):
                if isinstance(content, dict):
                    message = content
                else:
                    message += content
                    yield _emit_streaming_content(content)

            yield _emit_streaming_content("", last=True)
            yield _respond(_format_docs_message(message), overwrite=True)
        else:
            yield _respond(_format_docs_message(run_docs_query(query)))

        return
    elif intent == "computer_vision":
        if allow_streaming:
            message = ""
            for content in stream_computer_vision_query(query):
                message += content
                yield _emit_streaming_content(content)

            yield _emit_streaming_content("", last=True)
            yield _respond(message, overwrite=True)
        else:
            yield _respond(run_computer_vision_query(query))

        return
    elif intent != "display":
        yield _respond(_clarify_message())
        return

    if sample_collection is None:
        yield _respond(
            "You must provide a sample collection in order for me to respond "
            "to this query"
        )
        return

    if sample_collection.media_type not in ("image", "video"):
        yield _respond(
            "Only image or video collections are currently supported"
        )
        return

    # Algorithms
    algorithms = select_algorithms(query)
    if algorithms:
        yield _respond(_algorithms_message(algorithms))

    # Runs
    runs, runs_message = select_runs(sample_collection, query, algorithms)
    if runs or runs_message:
        yield _respond(_runs_message(runs, runs_message))

    # Fields
    fields = select_fields(sample_collection, query)
    if fields:
        yield _respond(_fields_message(fields))

    # Label classes
    label_classes = select_label_classes(sample_collection, query, fields)
    if label_classes == "_CONFUSED_":
        if "text_similarity" in runs:
            label_classes = {}
        else:
            yield _respond(_clarify_message())
            return

    if any(len(v) > 0 for v in label_classes.values()):
        _label_classes, _unmatched_classes = _format_label_classes(
            label_classes
        )
        yield _respond(_label_classes_message(_label_classes))

    examples = generate_view_stage_examples_prompt(
        sample_collection, query, runs, label_classes
    )
    view_stage_descriptions = generate_view_stage_descriptions_prompt(examples)

    # View stages
    view_stages = get_most_relevant_view_stages(examples)
    if view_stages:
        yield _respond(_view_stages_message(view_stages))

    examples = _reformat_query(examples, label_classes)
    _label_classes, _unmatched_classes = _format_label_classes(label_classes)

    stages = get_gpt_view_stage_strings(
        sample_collection,
        runs,
        fields,
        _label_classes,
        _unmatched_classes,
        view_stage_descriptions,
        examples,
    )

    if "metadata" in ".".join(stages) and "metadata" not in runs:
        stages = "_NEED_METADATA_"

    if dialect == "raw":
        yield stages
        return

    if stages == "_NEED_METADATA_":
        yield _respond(_metadata_message())
        return

    if stages == "_MORE_":
        yield _respond(_specific_message())
        return

    if stages == "_CONFUSED_":
        yield _respond(_clarify_message())
        return

    stages = _format_stages(sample_collection, stages)

    if stages:
        yield _respond(_load_view_message(stages))

        try:
            view = _build_view(sample_collection, stages)
            yield _emit_view(view)
        except Exception as e:
            yield _respond(_invalid_view_message())
    else:
        yield _respond(_full_collection_message(sample_collection))
        yield _emit_view(sample_collection)


def _format_label_classes(label_classes):
    unmatched_entities = []

    label_fields = list(label_classes.keys())
    label_class_dict = {}
    for field in label_fields:
        field_list = []
        lcl = label_classes[field]
        for el in lcl:
            el_val = list(el.values())[0]
            if type(el_val) == str:
                field_list.append(el_val)
            else:
                unmatched_entities.append(list(el.keys())[0])
                field_list += el_val

        label_class_dict[field] = field_list

    return label_class_dict, unmatched_entities


def _reformat_query(examples, label_classes):
    example_lines = examples.split("\n")
    query = example_lines[-2]

    label_classes_list = list(label_classes.values())
    label_classes_list = [
        item for sublist in label_classes_list for item in sublist
    ]
    class_name_map = {k: v for d in label_classes_list for k, v in d.items()}
    for k, v in class_name_map.items():
        if type(v) == str:
            query = query.replace(k, v)
        elif v:
            clarification = f" where by {k} I mean any of {v}"
            query += clarification

    example_lines[-2] = query
    return "\n".join(example_lines)


def _format_stages(sample_collection, stages):
    stages = [s for s in stages if s.strip() not in ("", "None")]

    if not stages:
        return None

    if isinstance(sample_collection, fo.DatasetView):
        ctype = "view"
    else:
        ctype = "dataset"

    return [ctype] + stages


def _build_view(sample_collection, stages):
    import fiftyone as fo
    from fiftyone import ViewField as F

    if isinstance(sample_collection, fo.DatasetView):
        view = sample_collection
        dataset = view._root_dataset
    else:
        dataset = sample_collection
        view = dataset.view()

    view = eval(".".join(stages))

    # Ensures view is valid
    _ = view.count()

    return view


def _log_chat_history(speaker, text, chat_history):
    chat_history.append(f"{speaker}: {text}")


def _moderation_fail_message():
    return "I'm sorry, this query does not abide by OpenAI's guidelines"


def _metadata_message():
    return {
        "string": "Please compute metadata and then try again",
        "markdown": (
            "Please run `compute_metadata()` on your samples and then try "
            "again"
        ),
    }


def _clarify_message():
    return "I'm sorry, I don't understand. Can you clarify what you're asking?"


def _specific_message():
    return "I'm sorry, I don't understand. Can you be more specific?"


def _format_docs_message(response):
    # Markdown
    # Convert all URLs to [url](url)
    patt = r"(https?://[^\s]+)"
    repl = r"[\1](\1)"
    md_response = re.sub(patt, repl, response)

    return {
        "string": response,
        "markdown": md_response,
    }


def _algorithms_message(algorithms):
    prefix = "Identified potential algorithms: "
    markdown = ", ".join(f"`{a}`" for a in algorithms)
    return {
        "string": prefix + ", ".join(algorithms),
        "markdown": prefix + markdown,
    }


def _runs_message(runs, runs_message):
    prefix = "Identified potential runs: "
    runs_map = {}
    for run_type in list(runs.keys()):
        run_info = runs[run_type]
        if run_type == "uniqueness":
            key = "uniqueness_field"
        else:
            key = "key"

        runs_map[run_type] = run_info[key]

    # String
    str_message = ""
    if runs_map:
        str_message += prefix + str(runs_map)

    if runs_message:
        str_message += "\n" + runs_message + "\n"

    # Markdown
    runs_md = ""
    if runs_map:
        chunks = []
        for run_type, key in runs_map.items():
            chunks.append(f"\n - `{run_type}` run: `{key}`")

        runs_md += prefix + "".join(chunks)

    if runs_message:
        runs_md += "\n" + runs_message + "\n"

    return {
        "string": str_message,
        "markdown": runs_md,
    }


def _fields_message(fields):
    prefix = "Identified potential fields: "
    markdown = ", ".join(f"`{f}`" for f in fields)
    return {
        "string": prefix + ", ".join(fields),
        "markdown": prefix + markdown,
    }


def _label_classes_message(label_classes):
    prefix = "Identified potential label classes: "

    # Markdown
    chunks = []
    for label_field, classes in label_classes.items():
        chunks.append(f"\n - `{label_field}` field: ")
        if classes:
            chunks.append(", ".join(f"`{c}`" for c in sorted(classes)))
        else:
            chunks.append("N/A")

    markdown = "".join(chunks)

    return {
        "string": prefix + str(label_classes),
        "markdown": prefix + markdown,
    }


_HELP_MESSAGE_MARKDOWN = """
Hi! I'm VoxelGPT, your AI assistant for computer vision.

I can help you with the following tasks:
- 🔎 **Querying your data:** I can help you filter, match, sort, and more - without writing a line of code. Tell me what you'd like to see and I'll load the corresponding view
- 💪 **Becoming a FiftyOne pro:** I have access to the FiftyOne documentation, so I can help you learn how to use FiftyOne and find the information you're looking for
- 📈 **Troubleshooting data quality:** I can help you build better datasets and higher quality models by answering general knowledge questions about computer vision and machine learning

**Tips**
- Be as specific as possible. The more specific you are, the better I can help you. I am still learning, so sometimes I need a little help understanding what you're asking
- If you want to query your dataset, but your input is being interpreted as a documentation or general computer vision query, try using the `show` keyword. For example: *"show me all images with a label of dog"*
- If you want to query the FiftyOne documentation, try using either `docs` or `fiftyone` in your query. For example: *"how do I load a dataset in FiftyOne?"*
- If you want me to use our conversation history to infer what you're asking, try using the `now` keyword. For example: if you just asked *"show me high confidence predictions of cats, dogs, and rabbits"*, you can ask *"now the low confidence predictions"*

**Learn more**
- You can learn more about me on my [GitHub page](https://github.com/voxel51/voxelgpt). While you're at it, please give me a star ⭐! VoxelGPT is open source and it is constantly improving. Contributions are welcome!
- Did you know that I'm a [FiftyOne Plugin](https://docs.voxel51.com/plugins/index.html)? Check out how FiftyOne can be extended to do all sorts of cool things!
- Learn more about [FiftyOne](https://github.com/voxel51/fiftyone) and give the project a star ⭐! FiftyOne is open source too!
- Join the [FiftyOne Slack community](https://slack.voxel51.com) where thousands of enthusiasts and professionals are discussing the latest in computer vision and machine learning

I'm still learning, so I appreciate your patience 😊
"""

_HELP_MESSAGE_STRING = """
Hi! I'm VoxelGPT, your AI assistant for computer vision.

I can help you with the following tasks
===============================================================================

🔎  ~~Querying your data~~
    I can help you filter, match, sort, and more - without writing a line of
    code. Tell me what you'd like to see and I'll load the corresponding view

💪  ~~Becoming a FiftyOne pro~~
    I have access to the FiftyOne documentation, so I can help you learn how to
    use FiftyOne and find the information you're looking for

📈  ~~Troubleshooting data quality~~
    I can help you build better datasets and higher quality models by answering
    general knowledge questions about computer vision and machine learning

Tips
===============================================================================

1.  ~~Be as specific as possible~~
    The more specific you are, the better I can help you. I am still learning,
    so sometimes I need a little help understanding what you're asking

2.  If you want to query your dataset, but your input is being interpreted as a
    documentation or general computer vision query, try using the 'show'
    keyword. For example:

        show me all images with a label of dog

3.  If you want to query the FiftyOne documentation, try using either 'docs' or
   'fiftyone' in your query. For example:

        how do I load a dataset in FiftyOne?

4.  If you want me to use our conversation history to infer what you're asking,
    try using the 'now' keyword. For example:

        show me high confidence predictions of cats, dogs, and rabbits
        now the low confidence predictions

5.  Type 'reset' to clear our chat history

6.  Type 'exit' to exit our chat

Learn more
===============================================================================

-   You can learn more about me on GitHub: https://github.com/voxel51/voxelgpt
    While you're at it, please give me a star ⭐! VoxelGPT is an open source
    project and it is constantly improving. Contributions are welcome!

-   Did you know that I'm a FiftyOne Plugin? Check out how FiftyOne can be 
    extended to do all sorts of cool things at https://docs.voxel51.com/plugins/index.html

-   Learn more about FiftyOne at https://github.com/voxel51/fiftyone
    Please give the project a star ⭐! FiftyOne is open source too!

-   Join the FiftyOne Slack community at https://slack.voxel51.com
    Thousands of enthusiasts and professionals are discussing the latest in
    computer vision and machine learning

I'm still learning, so I appreciate your patience 😊
"""


def _help_message():
    return {
        "string": _HELP_MESSAGE_STRING.strip(),
        "markdown": _HELP_MESSAGE_MARKDOWN.strip(),
    }


def _view_stages_message(view_stages):
    prefix = "Identified potential view stages: "
    markdown = ", ".join(_wrap_stage_name(name) for name in view_stages)
    return {
        "string": prefix + str(view_stages),
        "markdown": prefix + markdown,
    }


def _wrap_stage_name(stage_name):
    return f"[{stage_name}()]({_get_stage_doc_link(stage_name)})"


def _get_stage_doc_link(stage_name):
    return f"https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.{stage_name}"


def _load_view_message(stages):
    prefix = "Okay, I'm going to load "
    view_str = ".".join(stages)

    # Markdown
    if len(view_str) < 80 or len(stages) <= 2:
        markdown = f":\n```py\n{view_str}\n```"
    else:
        stages_str = "".join(f"    .{s}\n" for s in stages[1:])
        markdown = f":\n```py\nview = (\n    {stages[0]}\n{stages_str})\n```"

    return {
        "string": prefix + view_str,
        "markdown": prefix + markdown,
    }


def _invalid_view_message():
    return "I tested the view and it was invalid. Please try again"


def _full_collection_message(sample_collection):
    if isinstance(sample_collection, fo.DatasetView):
        ctype = "view"
    else:
        ctype = "dataset"

    return f"Okay, let's load your entire {ctype}"


def _emit_message(message, history, overwrite=False):
    return {
        "type": "message",
        "data": {
            "message": message,
            "history": history,
            "overwrite": overwrite,
        },
    }


def _emit_streaming_content(content, last=False):
    return {"type": "streaming", "data": {"content": content, "last": last}}


def _emit_view(view):
    return {"type": "view", "data": {"view": view}}


def _emit_warning(message):
    return {"type": "warning", "data": {"message": message}}
